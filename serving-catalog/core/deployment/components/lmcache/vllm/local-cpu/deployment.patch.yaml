apiVersion: apps/v1
kind: Deployment
metadata:
  name: "*"
spec:
  template:
    spec:
      containers:
      - name: inference-server
        env:
        - name: LMCACHE_MAX_LOCAL_CPU_SIZE
          value: "128.0"
        resources:
          requests:
            memory: 1024G
