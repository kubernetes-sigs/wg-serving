apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: llama3-8b-jetstream-inference-server
  name: llama3-8b-jetstream-deployment
spec:
  selector:
    matchLabels:
      app: llama3-8b-jetstream-inference-server
  template:
    metadata:
      labels:
        app: llama3-8b-jetstream-inference-server
    spec:
      containers:
      - name: inference-server
        image: us-docker.pkg.dev/cloud-tpu-images/inference/inference-checkpoint:v0.2.2
        ports:
          - containerPort: 9100
            name: metrics
        args:
        - --size=8b
        - --model_name=llama-3
        - --batch_size=80
        - --max_cache_length=2048
        - --quantize_weights=False
        - --quantize_kv_cache=False
        - --tokenizer_path=assets/tokenizer_llama3.tiktoken
        - --load_parameters_path=gs://$BUCKET_NAME/maxtext/llama3.1-8b/bf16/unscanned/checkpoints/0/items
        - --prometheus_port=9100
